{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "We'll use the Iris dataset from sklearn. This dataset has three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm)    0\n",
      "sepal width (cm)     0\n",
      "petal length (cm)    0\n",
      "petal width (cm)     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "print(df.isnull().sum())\n",
    "df[\"class\"] = y\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "val_df = pd.DataFrame(X_val, columns=feature_names)\n",
    "\n",
    "numerical_features = feature_names\n",
    "categorical_features = []\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Logistic Regression Model\n",
    "We'll define a multiclass Logistic Regression model using PyTorch. The model will have a linear layer that maps the input features to the class scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionLoss(nn.Module):\n",
    "    def __init__(self, model, lambd):\n",
    "        super(LogisticRegressionLoss, self).__init__()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.model = model\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # Compute the cross-entropy loss\n",
    "        ce_loss = self.cross_entropy(outputs, labels)\n",
    "\n",
    "        # Compute the L2 regularization term\n",
    "        l2_reg = 0.0\n",
    "        for param in self.model.parameters():\n",
    "            # l2_reg += torch.norm(param) ** 2\n",
    "            # l2_reg = l2_reg + torch.norm(param) ** 2\n",
    "            l2_reg = l2_reg + param.square().sum()\n",
    "\n",
    "        # Combine the cross-entropy loss and the L2 regularization term\n",
    "        # loss = ce_loss + self.lambd * l2_reg\n",
    "        loss = ce_loss + self.lambd * 100.0\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperTrainer:\n",
    "    def __init__(self, num_epochs, model, criterion, train_loader, hyper_optimizer):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.hyper_optimizer = hyper_optimizer\n",
    "        self.hyperparams_history = {\n",
    "            name: [hyperparam.item()]\n",
    "            for name, hyperparam in self.hyper_optimizer.hyperparams.items()\n",
    "        }\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in self.train_loader:\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward(retain_graph=True)\n",
    "\n",
    "                # Use the hyper optimizer step method\n",
    "                self.hyper_optimizer.step(loss)\n",
    "                if self.hyper_optimizer.step_count % self.hyper_optimizer.inner_steps == 0:\n",
    "                    for name, hyperparam in self.hyper_optimizer.hyperparams.items():\n",
    "                        self.hyperparams_history[name].append(hyperparam.item())\n",
    "\n",
    "                # Zero the gradients using the hyper optimizer\n",
    "                self.hyper_optimizer.zero_grad()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.num_epochs}, Training Loss: {running_loss/len(self.train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hippotrainer import Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage example\n",
    "# input_dim = len(feature_names)\n",
    "# num_classes = len(class_names)\n",
    "# model = LogisticRegressionModel(input_dim, num_classes)\n",
    "# num_epochs = 10\n",
    "# theta = torch.tensor([1.0], requires_grad=True)\n",
    "# lambd = theta.exp()\n",
    "\n",
    "# # Initialize the custom loss function\n",
    "# criterion = LogisticRegressionLoss(model, lambd)\n",
    "\n",
    "# # Initialize the optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Initialize the hyper optimizer\n",
    "# hyper_optimizer = Direct(\n",
    "#     hyperparams={\"theta\": theta},\n",
    "#     hyper_lr=1e-3,\n",
    "#     inner_steps=5,\n",
    "#     model=model,\n",
    "#     optimizer=optimizer,\n",
    "#     val_loader=val_loader,\n",
    "#     criterion=criterion,\n",
    "# )\n",
    "\n",
    "# # Initialize the trainer\n",
    "# trainer = HyperTrainer(num_epochs, model, criterion, train_loader, hyper_optimizer)\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Evaluate the model\n",
    "# final_val_loss = hyper_optimizer.evaluate()\n",
    "# print(f\"Final Validation Loss: {final_val_loss.item():.4f}\")\n",
    "\n",
    "# # Plot the hyperparameter changing\n",
    "# for name, hyperparam_values in trainer.hyperparams_history.items():\n",
    "#     steps = list(range(len(hyperparam_values)))\n",
    "#     plt.plot(steps, hyperparam_values, marker=\"o\")\n",
    "#     plt.xlabel(\"Steps\")\n",
    "#     plt.ylabel(f\"Hyperparameter {name} Value\")\n",
    "#     plt.title(f\"Change of Hyperparameter {name} over Steps\")\n",
    "#     plt.grid(True)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neumann (Optimizing Millions of Hyperparameters by Implicit Diï¬€erentiation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hippotrainer import Neumann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J (tensor([[-0.7119, -0.4844, -0.0595,  0.0265],\n",
      "        [ 0.6539,  0.4520,  0.0335, -0.0325],\n",
      "        [ 0.0580,  0.0323,  0.0260,  0.0060]]), tensor([-0.1571,  0.1461,  0.0111]))\n",
      "J[0] tensor([[-0.7119, -0.4844, -0.0595,  0.0265],\n",
      "        [ 0.6539,  0.4520,  0.0335, -0.0325],\n",
      "        [ 0.0580,  0.0323,  0.0260,  0.0060]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got builtin_function_or_method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m final_val_loss \u001b[38;5;241m=\u001b[39m hyper_optimizer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[0;32mIn[51], line 24\u001b[0m, in \u001b[0;36mHyperTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Use the hyper optimizer step method\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_optimizer\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_optimizer\u001b[38;5;241m.\u001b[39minner_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, hyperparam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_optimizer\u001b[38;5;241m.\u001b[39mhyperparams\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/nkiselev/studying/hippotrainer/notebooks/../src/hippotrainer/hyper_optimizer.py:59\u001b[0m, in \u001b[0;36mHyperOptimizer.step\u001b[0;34m(self, train_loss)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyperparams_requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyperparams_requires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/nkiselev/studying/hippotrainer/notebooks/../src/hippotrainer/hyper_optimizer.py:65\u001b[0m, in \u001b[0;36mHyperOptimizer.hyper_step\u001b[0;34m(self, train_loss)\u001b[0m\n\u001b[1;32m     63\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, hyperparam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparams\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 65\u001b[0m     hyper_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     hyperparam\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_lr \u001b[38;5;241m*\u001b[39m hyper_grad\n",
      "File \u001b[0;32m~/nkiselev/studying/hippotrainer/notebooks/../src/hippotrainer/neumann.py:27\u001b[0m, in \u001b[0;36mNeumann.hyper_grad\u001b[0;34m(self, train_loss, val_loss, hyperparam)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m, J)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m, J[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 27\u001b[0m J \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mJ\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m v_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapprox_inverse_hvp(v_1, train_loss)\n\u001b[1;32m     29\u001b[0m v_3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(grad, hyperparam, grad_outputs\u001b[38;5;241m=\u001b[39mv_2)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got builtin_function_or_method"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "input_dim = len(feature_names)\n",
    "num_classes = len(class_names)\n",
    "model = LogisticRegressionModel(input_dim, num_classes)\n",
    "num_epochs = 10\n",
    "# theta = torch.tensor([1.0], requires_grad=True)\n",
    "# lambd = theta.exp()\n",
    "lambd = torch.tensor([1.0])\n",
    "\n",
    "# Initialize the custom loss function\n",
    "criterion = LogisticRegressionLoss(model, lambd)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize the hyper optimizer\n",
    "hyper_optimizer = Neumann(\n",
    "    hyperparams={\"theta\": theta},\n",
    "    hyper_lr=1e-3,\n",
    "    inner_steps=5,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = HyperTrainer(num_epochs, model, criterion, train_loader, hyper_optimizer)\n",
    "\n",
    "# Train the model\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "final_val_loss = hyper_optimizer.evaluate()\n",
    "print(f\"Final Validation Loss: {final_val_loss.item():.4f}\")\n",
    "\n",
    "# Plot the hyperparameter changing\n",
    "for name, hyperparam_values in trainer.hyperparams_history.items():\n",
    "    steps = list(range(len(hyperparam_values)))\n",
    "    plt.plot(steps, hyperparam_values, marker=\"o\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(f\"Hyperparameter {name} Value\")\n",
    "    plt.title(f\"Change of Hyperparameter {name} over Steps\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1-T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hippotrainer import T1T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     30\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m final_val_loss \u001b[38;5;241m=\u001b[39m hyper_optimizer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[0;32mIn[51], line 24\u001b[0m, in \u001b[0;36mHyperTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Use the hyper optimizer step method\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_optimizer\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_optimizer\u001b[38;5;241m.\u001b[39minner_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, hyperparam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_optimizer\u001b[38;5;241m.\u001b[39mhyperparams\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/nkiselev/studying/hippotrainer/notebooks/../src/hippotrainer/hyper_optimizer.py:59\u001b[0m, in \u001b[0;36mHyperOptimizer.step\u001b[0;34m(self, train_loss)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyperparams_requires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hyperparams_requires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/nkiselev/studying/hippotrainer/notebooks/../src/hippotrainer/hyper_optimizer.py:65\u001b[0m, in \u001b[0;36mHyperOptimizer.hyper_step\u001b[0;34m(self, train_loss)\u001b[0m\n\u001b[1;32m     63\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, hyperparam \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyperparams\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 65\u001b[0m     hyper_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyper_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     hyperparam\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhyper_lr \u001b[38;5;241m*\u001b[39m hyper_grad\n",
      "File \u001b[0;32m~/nkiselev/studying/hippotrainer/notebooks/../src/hippotrainer/t1_t2.py:15\u001b[0m, in \u001b[0;36mT1T2.hyper_grad\u001b[0;34m(self, train_loss, val_loss, hyperparam)\u001b[0m\n\u001b[1;32m     11\u001b[0m grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[1;32m     12\u001b[0m     train_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     14\u001b[0m v_2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(grad, hyperparam, grad_outputs\u001b[38;5;241m=\u001b[39mv_1, allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m hyper_grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mv_2\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hyper_grad\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "input_dim = len(feature_names)\n",
    "num_classes = len(class_names)\n",
    "model = LogisticRegressionModel(input_dim, num_classes)\n",
    "num_epochs = 10\n",
    "# theta = torch.tensor([1.0], requires_grad=True)\n",
    "# lambd = theta.exp()\n",
    "lambd = torch.tensor([1.0])\n",
    "\n",
    "# Initialize the custom loss function\n",
    "criterion = LogisticRegressionLoss(model, lambd)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Initialize the hyper optimizer\n",
    "hyper_optimizer = T1T2(\n",
    "    hyperparams={\"theta\": theta},\n",
    "    hyper_lr=1e-3,\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = HyperTrainer(num_epochs, model, criterion, train_loader, hyper_optimizer)\n",
    "\n",
    "# Train the model\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "final_val_loss = hyper_optimizer.evaluate()\n",
    "print(f\"Final Validation Loss: {final_val_loss.item():.4f}\")\n",
    "\n",
    "# Plot the hyperparameter changing\n",
    "for name, hyperparam_values in trainer.hyperparams_history.items():\n",
    "    steps = list(range(len(hyperparam_values)))\n",
    "    plt.plot(steps, hyperparam_values, marker=\"o\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(f\"Hyperparameter {name} Value\")\n",
    "    plt.title(f\"Change of Hyperparameter {name} over Steps\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nkiselev-hippotrainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
